#!/bin/bash

#PBS -N build_tokenizer
#PBS -q YOUR_QUEUE_NAME
#PBS -l select=1:ncpus=YOUR_NCPUS:mem=YOUR_MEMORY
#PBS -l walltime=YOUR_WALLTIME
#PBS -j oe
#PBS -o /path/to/your/logs/

# ============================================================================
# PBS Job Script for Building Tokenizer Expansions - TEMPLATE
# ============================================================================
#
# This script builds tokenizer expansions for both Stochastok and Patok:
#   - Stochastok: Simple stochastic token expansion
#   - Patok: Morphology-aware token expansion for Filipino
#
# This is a TEMPLATE file. Copy to jobs/ and customize:
#   1. Replace YOUR_QUEUE_NAME with your cluster queue
#   2. Replace /path/to/your/logs/ with your log directory
#   3. Replace YOUR_NCPUS (typically 4-8)
#   4. Replace YOUR_MEMORY (typically 32GB-64GB for large tokenizers)
#   5. Replace YOUR_WALLTIME (e.g., 08:00:00)
#   6. Replace /path/to/your/project with your project directory
#
# Usage:
#   cp job_templates/build_tokenizer_expansions.template.pbs jobs/build_tokenizer_expansions.pbs
#   vim jobs/build_tokenizer_expansions.pbs  # Customize
#   qsub -v TOKENIZER_NAME=gpt2 jobs/build_tokenizer_expansions.pbs
#   qsub -v TOKENIZER_NAME=google/gemma-3-1b-pt jobs/build_tokenizer_expansions.pbs
#
# ============================================================================

# Change to project directory
cd /path/to/your/project

# Activate conda/virtual environment
source env/bin/activate

# Check for required parameter
if [ -z "$TOKENIZER_NAME" ]; then
    echo "Error: TOKENIZER_NAME not set"
    echo "Usage: qsub -v TOKENIZER_NAME=gpt2 jobs/build_tokenizer_expansions.pbs"
    echo "   or: qsub -v TOKENIZER_NAME=google/gemma-3-1b-pt jobs/build_tokenizer_expansions.pbs"
    exit 1
fi

# Run the expansion building script
python scripts/build_tokenizer_expansions.py "$TOKENIZER_NAME"
