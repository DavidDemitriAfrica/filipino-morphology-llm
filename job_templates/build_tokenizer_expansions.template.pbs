#!/bin/bash

#PBS -N build_tokenizer
#PBS -q YOUR_QUEUE_NAME
#PBS -l select=1:ncpus=YOUR_NCPUS:mem=YOUR_MEMORY
#PBS -l walltime=YOUR_WALLTIME
#PBS -j oe
#PBS -o /path/to/your/logs/

# ============================================================================
# PBS Job Script for Building Tokenizer Expansions - TEMPLATE
# ============================================================================
#
# This script builds BOTH Stochastok and Patok tokenizer expansions:
#   - Stochastok: Simple stochastic token expansion
#   - Patok: Morphology-aware token expansion for Filipino
#
# This is a TEMPLATE file. Copy to jobs/ and customize:
#   1. Replace YOUR_QUEUE_NAME with your cluster queue
#   2. Replace /path/to/your/logs/ with your log directory
#   3. Replace YOUR_NCPUS (typically 4)
#   4. Replace YOUR_MEMORY (typically 32GB)
#   5. Replace YOUR_WALLTIME (e.g., 08:00:00)
#
# Usage:
#   cp job_templates/build_tokenizer_expansions.template.pbs jobs/build_tokenizer_expansions.pbs
#   vim jobs/build_tokenizer_expansions.pbs  # Customize
#   qsub -v TOKENIZER_NAME=gpt2 jobs/build_tokenizer_expansions.pbs
#   qsub -v TOKENIZER_NAME=google/gemma-3-1b-pt jobs/build_tokenizer_expansions.pbs
#
# ============================================================================

# Load environment variables
cd /path/to/your/project
source .env

# Activate conda environment
source env/bin/activate

# Check for required parameter
if [ -z "$TOKENIZER_NAME" ]; then
    echo "Error: TOKENIZER_NAME not set"
    echo "Usage: qsub -v TOKENIZER_NAME=gpt2 jobs/build_tokenizer_expansions.pbs"
    echo "   or: qsub -v TOKENIZER_NAME=google/gemma-3-1b-pt jobs/build_tokenizer_expansions.pbs"
    exit 1
fi

echo "================================================================"
echo "Building Tokenizer Expansions (Stochastok + Patok)"
echo "================================================================"
echo "Tokenizer: $TOKENIZER_NAME"
echo "Start time: $(date)"
echo "================================================================"

# Build the expansions
python3 << EOF
import sys
import os
sys.path.insert(0, 'src')

from transformers import AutoTokenizer
from tokenization import StochastokProcessor, MorphologyAwarePatokProcessor
from tokenization.base_processor import TokenizerProcessor

tokenizer_name = "$TOKENIZER_NAME"
print(f"Loading tokenizer: {tokenizer_name}")
tokenizer = AutoTokenizer.from_pretrained(tokenizer_name)
print(f"✓ Loaded tokenizer: {type(tokenizer).__name__}")
print(f"  Vocab size: {tokenizer.vocab_size}")

# Get sanitized tokenizer name for checking existing files
base_processor = TokenizerProcessor(tokenizer)
sanitized_name = base_processor.tokenizer_name
print(f"  Sanitized name: {sanitized_name}")
print()

# Check if expansions already exist
stochastok_path = base_processor.get_cache_path("expansions_stochastok", f"expansions_stochastok_{sanitized_name}.json")
patok_path = base_processor.get_cache_path("expansions_patok", f"expansions_patok_{sanitized_name}.json")

stochastok_exists = os.path.exists(stochastok_path)
patok_exists = os.path.exists(patok_path)

if stochastok_exists and patok_exists:
    print("=" * 70)
    print("✓ Expansions already exist for this tokenizer!")
    print("=" * 70)
    print(f"Stochastok: {stochastok_path}")
    print(f"Patok:      {patok_path}")
    print()
    print("Skipping build. To rebuild, delete the existing files.")
    sys.exit(0)
elif stochastok_exists:
    print(f"Note: Stochastok expansions already exist at {stochastok_path}")
elif patok_exists:
    print(f"Note: Patok expansions already exist at {patok_path}")
print()

# Build Stochastok expansions
print("=" * 70)
print("Building Stochastok expansions...")
print("=" * 70)
print("(This may take several hours for large vocabularies)")
stochastok_processor = StochastokProcessor(tokenizer, expand_prop=0.1)
print()
print("✓ Stochastok expansions built successfully")
print(f"  Number of expansions: {len(stochastok_processor.expansions)}")
print()

# Build Patok expansions
print("=" * 70)
print("Building Patok expansions...")
print("=" * 70)
print("(This may take several hours for large vocabularies)")

# Use Filipino affix files
patok_processor = MorphologyAwarePatokProcessor(
    tokenizer,
    prefix_file="data/affixes_filipino/prefix.txt",
    infix_file="data/affixes_filipino/infix.txt",
    suffix_file="data/affixes_filipino/suffix.txt",
)
print()
print("✓ Patok expansions built successfully")
print(f"  Number of expansions: {len(patok_processor.expansions)}")
print()

print("=" * 70)
print("All expansions built and saved!")
print("=" * 70)
EOF

echo "================================================================"
echo "Complete!"
echo "End time: $(date)"
echo "================================================================"
